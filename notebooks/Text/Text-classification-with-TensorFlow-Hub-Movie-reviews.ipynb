{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification with TensorFlow Hub: Movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook classifies movie reviews as positive or negative using the text of the review. This is an example of binary classification.\n",
    "\n",
    "For this example we will use `IMDB dataset` that contains the text of 50,000 movie reviews from the Internet Movie Database. These are split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are balanced, meaning they contain an equal number of positive and negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.0.0\n",
      "Eager mode:  True\n",
      "Hub version:  0.8.0\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset imdb_reviews/plain_text/1.0.0 (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6fbaee0eab742a788aebed17e200675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69bd7adb277444858a07737fc4fd59e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteTWBQSH/imdb_reviews-train.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9209c62b2564983bd742efeb5aebc76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteTWBQSH/imdb_reviews-test.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f7edfc982c44523b50b06e783a852e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteTWBQSH/imdb_reviews-unsupervised.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b97ad1f473114064abeb1744e48ff165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_data, validation_data, test_data = tfds.load(name=\"imdb_reviews\", \n",
    "                                                   split=('train[:60%]', 'train[60%:]', 'test'),\n",
    "                                                   as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the data\n",
    "\n",
    "Each example is a sentence representing the movie review and a corresponding label. The sentence is not preprocessed in any way. The label is an integer value of either 0 or 1, where 0 is a negative review, and 1 is a positive review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train example batch:  tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string)\n",
      "\n",
      "Train labels batch:  tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "train_examples_batch, train_labels_batch = next(iter(train_data.batch(10)))\n",
    "print(\"Train example batch: \", train_examples_batch[0])\n",
    "print()\n",
    "print(\"Train labels batch: \", train_labels_batch[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the model\n",
    "\n",
    "One way to represent the text is to convert sentences into embeddings vectors. We can use a pre-trained text embedding as the first layer, which will have three advantages:\n",
    "\n",
    "  * we don't have to worry about text preprocessing,\n",
    "  * we can benefit from transfer learning,\n",
    "  * the embedding has a fixed size, so it's simpler to process.\n",
    "  \n",
    "For this example we will use a pre-trained text embedding model from TensorFlow Hub called `google/tf2-preview/gnews-swivel-20dim/1`.\n",
    "\n",
    "Other three available embeddings that could be used in this tutorial are:\n",
    "\n",
    "  * `google/tf2-preview/gnews-swivel-20dim-with-oov/1`- vocabulary contains OOV (Out of vocabulary) buckets. This can help if vocabulary of the task and vocabulary of the model don't fully overlap.\n",
    "  * `google/tf2-preview/nnlm-en-dim50/1` - A much larger model with ~1M vocabulary size and 50 dimensions.\n",
    "  * `google/tf2-preview/nnlm-en-dim128/1` - Even larger model with ~1M vocabulary size and 128 dimensions.\n",
    " \n",
    "The output shape of the embeddings is `(num_examples, embedding_dimension)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=469, shape=(3, 20), dtype=float32, numpy=\n",
       "array([[ 1.765786  , -3.882232  ,  3.9134233 , -1.5557289 , -3.3362343 ,\n",
       "        -1.7357955 , -1.9954445 ,  1.2989551 ,  5.081598  , -1.1041286 ,\n",
       "        -2.0503852 , -0.72675157, -0.65675956,  0.24436149, -3.7208383 ,\n",
       "         2.0954835 ,  2.2969332 , -2.0689783 , -2.9489717 , -1.1315987 ],\n",
       "       [ 1.8804485 , -2.5852382 ,  3.4066997 ,  1.0982676 , -4.056685  ,\n",
       "        -4.891284  , -2.785554  ,  1.3874227 ,  3.8476458 , -0.9256538 ,\n",
       "        -1.896706  ,  1.2113281 ,  0.11474707,  0.76209456, -4.8791065 ,\n",
       "         2.906149  ,  4.7087674 , -2.3652055 , -3.5015898 , -1.6390051 ],\n",
       "       [ 0.71152234, -0.6353217 ,  1.7385626 , -1.1168286 , -0.5451594 ,\n",
       "        -1.1808156 ,  0.09504455,  1.4653089 ,  0.66059524,  0.79308075,\n",
       "        -2.2268345 ,  0.07446612, -1.4075904 , -0.70645386, -1.907037  ,\n",
       "         1.4419787 ,  1.9551861 , -0.42660055, -2.8022065 ,  0.43727064]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "hub_layer = hub.KerasLayer(embedding, \n",
    "                           input_shape=[],\n",
    "                           dtype=tf.string,\n",
    "                           trainable=True)\n",
    "hub_layer(train_examples_batch[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer (KerasLayer)     (None, 20)                400020    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                336       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 400,373\n",
      "Trainable params: 400,373\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and optimizer\n",
    "\n",
    "A model needs a loss function and an optimizer for training. Since this is a binary classification problem and the model outputs a probability (a single-unit layer with a sigmoid activation), we'll use the `binary_crossentropy` loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "30/30 [==============================] - 3s 112ms/step - loss: 0.7313 - accuracy: 0.5173 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 0.6409 - accuracy: 0.6065 - val_loss: 0.6073 - val_accuracy: 0.6333\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 2s 75ms/step - loss: 0.5842 - accuracy: 0.6653 - val_loss: 0.5628 - val_accuracy: 0.6879\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 0.5402 - accuracy: 0.7077 - val_loss: 0.5248 - val_accuracy: 0.7160\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 0.4983 - accuracy: 0.7430 - val_loss: 0.4902 - val_accuracy: 0.7539\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 0.4616 - accuracy: 0.7726 - val_loss: 0.4581 - val_accuracy: 0.7787\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 0.4238 - accuracy: 0.8023 - val_loss: 0.4290 - val_accuracy: 0.7879\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 0.3892 - accuracy: 0.8241 - val_loss: 0.4019 - val_accuracy: 0.8107\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 0.3541 - accuracy: 0.8421 - val_loss: 0.3803 - val_accuracy: 0.8182\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 2s 77ms/step - loss: 0.3258 - accuracy: 0.8581 - val_loss: 0.3604 - val_accuracy: 0.8342\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 2s 75ms/step - loss: 0.3005 - accuracy: 0.8749 - val_loss: 0.3450 - val_accuracy: 0.8429\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 2s 76ms/step - loss: 0.2755 - accuracy: 0.8867 - val_loss: 0.3326 - val_accuracy: 0.8524\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 2s 75ms/step - loss: 0.2568 - accuracy: 0.8972 - val_loss: 0.3241 - val_accuracy: 0.8504\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 2s 75ms/step - loss: 0.2377 - accuracy: 0.9076 - val_loss: 0.3148 - val_accuracy: 0.8608\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 2s 75ms/step - loss: 0.2219 - accuracy: 0.9153 - val_loss: 0.3088 - val_accuracy: 0.8623\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 2s 76ms/step - loss: 0.2071 - accuracy: 0.9227 - val_loss: 0.3043 - val_accuracy: 0.8631\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 0.1934 - accuracy: 0.9303 - val_loss: 0.3012 - val_accuracy: 0.8648\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 0.1799 - accuracy: 0.9348 - val_loss: 0.2993 - val_accuracy: 0.8656\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 0.1698 - accuracy: 0.9399 - val_loss: 0.2979 - val_accuracy: 0.8691\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 0.1563 - accuracy: 0.9459 - val_loss: 0.2981 - val_accuracy: 0.8682\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data.shuffle(10000).batch(512),\n",
    "                    epochs=20,\n",
    "                    validation_data=validation_data.batch(512),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 - 2s - loss: 0.3158 - accuracy: 0.8598\n",
      "loss: 0.316\n",
      "accuracy: 0.860\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_data.batch(512), verbose=2)\n",
    "\n",
    "for name, value in zip(model.metrics_names, results):\n",
    "  print(\"%s: %.3f\" % (name, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
