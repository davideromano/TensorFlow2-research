{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings give us a way to use an efficient, dense representation in which similar words have a similar encoding. Importantly, we do not have to specify this encoding by hand. An embedding is a dense vector of floating point values (the length of the vector is a parameter you specify).\n",
    "\n",
    "It is common to see word embeddings that are 8-dimensional (for small datasets), up to 1024-dimensions when working with large datasets. A higher dimensional embedding can capture fine-grained relationships between words, but takes more data to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../data/pictures/embedding2.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = layers.Embedding(1000, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you create an Embedding layer, the weights for the embedding are randomly initialized (just like any other layer). During training, they are gradually adjusted via backpropagation. Once trained, the learned word embeddings will roughly encode similarities between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=27, shape=(3, 5), dtype=float32, numpy=\n",
       "array([[-0.02030228, -0.00625715,  0.03203246, -0.00734886, -0.03341927],\n",
       "       [ 0.04393892, -0.0441018 ,  0.04863973, -0.04373846, -0.01031322],\n",
       "       [ 0.03667463,  0.02193252,  0.04739991,  0.0125013 ,  0.04758617]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer(tf.constant([1,2,999]).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For text or sequence problems, the Embedding layer takes a 2D tensor of integers, of shape `(samples, sequence_length)`, where each entry is a sequence of integers.\n",
    "\n",
    "The returned tensor has one more axis than the input, the embedding vectors are aligned along the new last axis. Pass it a `(2, 3)` input batch and the output is `(2, 3, N)`: `(samples, sequence_length, embedding_dimensionality)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = embedding_layer(tf.constant([[0,1,2],[3,4,5]]))\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn embeddings from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, test_data), info = tfds.load('imdb_reviews/subwords8k', \n",
    "                                          split = (tfds.Split.TRAIN, tfds.Split.TEST), \n",
    "                                          with_info=True,\n",
    "                                          as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the_',\n",
       " ', ',\n",
       " '. ',\n",
       " 'a_',\n",
       " 'and_',\n",
       " 'of_',\n",
       " 'to_',\n",
       " 's_',\n",
       " 'is_',\n",
       " 'br',\n",
       " 'in_',\n",
       " 'I_',\n",
       " 'that_',\n",
       " 'this_',\n",
       " 'it_',\n",
       " ' /><',\n",
       " ' />',\n",
       " 'was_',\n",
       " 'The_',\n",
       " 'as_']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = info.features['text'].encoder\n",
    "encoder.subwords[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = train_data.shuffle(1000).padded_batch(10, padded_shapes=([None],[]))\n",
    "test_batches = test_data.shuffle(1000).padded_batch(10, padded_shapes=([None],[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 444,   18,  122, ...,    0,    0,    0],\n",
       "       [2307, 1031,  800, ...,    0,    0,    0],\n",
       "       [ 156,   37, 1167, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [1071,    2,    4, ...,    0,    0,    0],\n",
       "       [  62,    9,  281, ...,    0,    0,    0],\n",
       "       [  62,   27,   18, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch, train_labels = next(iter(train_batches))\n",
    "train_batch.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 16)          130960    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 131,249\n",
      "Trainable params: 131,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 16\n",
    "\n",
    "model = keras.Sequential([\n",
    "  layers.Embedding(encoder.vocab_size, embedding_dim),\n",
    "  layers.GlobalAveragePooling1D(),\n",
    "  layers.Dense(16, activation='relu'),\n",
    "  layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='/tf/tb_logs')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2500/2500 [==============================] - 8s 3ms/step - loss: 0.0270 - accuracy: 0.9894 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.0190 - accuracy: 0.9937 - val_loss: 2.3040 - val_accuracy: 0.8300\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.0184 - accuracy: 0.9934 - val_loss: 2.2507 - val_accuracy: 0.8250\n",
      "Epoch 4/10\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.0186 - accuracy: 0.9938 - val_loss: 2.2563 - val_accuracy: 0.8200\n",
      "Epoch 5/10\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.0185 - accuracy: 0.9934 - val_loss: 1.8160 - val_accuracy: 0.8300\n",
      "Epoch 6/10\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.0189 - accuracy: 0.9934 - val_loss: 2.6720 - val_accuracy: 0.8100\n",
      "Epoch 7/10\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.0199 - accuracy: 0.9932 - val_loss: 2.5373 - val_accuracy: 0.8200\n",
      "Epoch 8/10\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.0194 - accuracy: 0.9934 - val_loss: 1.4705 - val_accuracy: 0.8650\n",
      "Epoch 9/10\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.0169 - accuracy: 0.9940 - val_loss: 1.8333 - val_accuracy: 0.8200\n",
      "Epoch 10/10\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.0168 - accuracy: 0.9944 - val_loss: 2.8653 - val_accuracy: 0.8000\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_batches,\n",
    "    epochs=10,\n",
    "    validation_data=test_batches,\n",
    "    validation_steps=20,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the model is overfitting (llok at the difference between the `validation loss` and the `training loss`.\n",
    "\n",
    "But in this case we are interested in the embadding layer that we just trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve the learned embeddings\n",
    "\n",
    "Next, let's retrieve the word embeddings learned during training. This will be a matrix of shape `(vocab_size, embedding-dimension)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (vocab_size, embedding_dim)\n",
      "(8185, 16)\n"
     ]
    }
   ],
   "source": [
    "weights = model.layers[0].get_weights()[0]\n",
    "print('shape: (vocab_size, embedding_dim)')\n",
    "print(weights.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use Tensorboard Projector plugin to plot the embeddings space, please refer to this [Github issue](https://github.com/tensorflow/tensorboard/issues/2471#issuecomment-580423961)\n",
    "\n",
    "(The Tensorflow documentation is not so good on this part)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
